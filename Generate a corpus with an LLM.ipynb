{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d7db3e",
   "metadata": {},
   "source": [
    "#  Generate a corpus with an LLM\n",
    "\n",
    "The last cells in the notebook allow you to delete a corpus or zip a corpus to download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dff11",
   "metadata": {},
   "source": [
    "Run this cell to install a specific Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c330702",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0a100",
   "metadata": {},
   "source": [
    "Run the following cell to import relevant Python libraries used in this notebook and set the logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import getpass\n",
    "from slugify import slugify\n",
    "import shutil\n",
    "import os\n",
    "import csv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89c27",
   "metadata": {},
   "source": [
    "Configure the key ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572823a2",
   "metadata": {},
   "source": [
    "The following cell contains a function to query Open Router and generate LLM text. Just run it to make the function available. Change it if you know what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt:str, # prompt to send to LLM\n",
    "            model: str, # model name e.g. google/gemma-2-9b-it:free\n",
    "            system_prompt: str = None, # system prompt to send to LLM\n",
    "            max_tokens: int = 2048, # maximum number of tokens to generate (includes prompt tokens)\n",
    "            response_format: str = None, # response format: json or None\n",
    "            temperature: float = None # temperature for sampling\n",
    "            ) -> requests.models.Response:\n",
    "    \"\"\" Query LLM with prompt \"\"\"\n",
    "\n",
    "    api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    if OPENROUTER_API_KEY is None:\n",
    "        logging.error(\"OPENROUTER_API_KEY not set. Not querying llm.\")\n",
    "        return None\n",
    "    api_key = OPENROUTER_API_KEY\n",
    "    \n",
    "    if prompt.strip() == '':\n",
    "        logging.error('No prompt provided. Not querying llm.')\n",
    "        return None\n",
    "    \n",
    "    messages = []\n",
    "    if system_prompt is not None and system_prompt.strip() != '':\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    request_data = {\n",
    "                \"model\": model, \n",
    "                \"messages\": messages,\n",
    "                'max_tokens': max_tokens\n",
    "                    }\n",
    "\n",
    "    if temperature is not None:\n",
    "        request_data['temperature'] = temperature\n",
    "    \n",
    "    if response_format == \"json\":\n",
    "        request_data['response_format'] = {\"type\": \"json_object\"}\n",
    "        \n",
    "    text = None\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "            },\n",
    "            data=json.dumps(request_data)\n",
    "            )\n",
    "        response.raise_for_status() \n",
    "        text = response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619145b",
   "metadata": {},
   "source": [
    "## Note about Open Router models (and set a model)\n",
    "\n",
    "While I've been using Open Router the last few days, sometimes I get some unhelpful errors back from their API if one of the free models is unavailable. If you get errors querying Open Router you can look up the message or error codes in their documentation. Often if you get an error on one model, another will work fine. Here is where you can set the model to use for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2237b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/llama-3-8b-instruct:free'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb64994",
   "metadata": {},
   "source": [
    "## Example 1 - generate a corpus with one LLM prompt\n",
    "\n",
    "This is a very basic example that uses one single prompt to generate a small corpus. Example 2 and 3 will probably be more helpful for your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4de5b0",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example1-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9da603",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae861b",
   "metadata": {},
   "source": [
    "Below are the settings we will use to generate a tiny corpus of three documents. \n",
    "\n",
    "***Warning:***\n",
    "\n",
    "You can change the number of texts to generate, but DON'T do this during the lab times as this may affect the classes ability to run the notebook. Remember there are limits (200 requests for free models per day, and a maximum of 20 requests per minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e725fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_texts_to_generate = 3 # PLEASE LEAVE THIS FOR NOW!\n",
    "\n",
    "system_prompt = '''\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "Write a short children's story imagining the future with AI.\n",
    "'''\n",
    "\n",
    "max_tokens = 1024\n",
    "\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9de68",
   "metadata": {},
   "source": [
    "This queries the API and generates a text, displays a preview of each generated text, and then saves the generated text as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_texts_to_generate):\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "    \n",
    "    filename = f'text-{i}.txt'\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('------')\n",
    "        \n",
    "    time.sleep(10) # always leave a delay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe97679",
   "metadata": {},
   "source": [
    "Inspect your txt files in the directory path you specified above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b7120",
   "metadata": {},
   "source": [
    "## Example 2: generate a corpus by seeding the prompt with some other generated data\n",
    "\n",
    "Here we first generate some data, and then we use this data to prompt the LLM. Here it is simply a title, but this could be more complex. For example, perhaps you generate the name of a person and a life history as the basis for a generated biography. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a3a8f",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d50882",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example2-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00573f5a",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00822a",
   "metadata": {},
   "source": [
    "First, we are going to generate some data that we will then use to generate the corpus. In this case we are going to generate some titles of opinion pieces on AI. Note: here I am generating JSON data. Note: there is a system prompt advising JSON output is required and the user prompt includes the required format. This sometimes may not work with smaller models! If it doesn't, just run it again. If you wanted to generate more titles you would change the number in the user prompt. For now, leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd076276",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Always respond with JSON data.\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "Generate a list of 3 editorial titles about artificial intelligence. \n",
    "Vary the length and theme of each title.\n",
    "JSON format: \n",
    "{\n",
    "    \"titles\": [\n",
    "        \"title 1\",\n",
    "        \"title 2\",\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "max_tokens = 8096\n",
    "\n",
    "response_format = 'json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17cdd",
   "metadata": {},
   "source": [
    "Query the API to generate the titles ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae467915",
   "metadata": {},
   "source": [
    "If the next cell runs ok you have some valid JSON. If not, run the cell above again. Sometimes LLMs create don't follow instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data = json.loads(response)\n",
    "for seed in seed_data['titles']:\n",
    "    print(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc50ba7",
   "metadata": {},
   "source": [
    "Now we will generate a tiny corpus of three documents based on these titles.  \n",
    "Note: we are specifying a system prompt, but there is no user prompt.  \n",
    "The prompt for each story is the title we have generated above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d65e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Write an editorial based on the title provided. The editorial should be written for a general audience.\n",
    "It will appear in a major news outlet. Do not include the title as part of the output.\n",
    "'''\n",
    "\n",
    "max_tokens = 2048\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888096bf",
   "metadata": {},
   "source": [
    "Run this to generate the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303753ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in seed_data['titles']:\n",
    "    print(f'Generating text based on: {prompt}')\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "\n",
    "    filename = slugify(prompt) + '.txt' # Note: this creates a nice filename from the title\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('-----------------')\n",
    "    time.sleep(10) # always use a time delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17b34b",
   "metadata": {},
   "source": [
    "Inspect your txt files in the directory path you specified above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c8760",
   "metadata": {},
   "source": [
    "## Example 3: generate a corpus by with a prompt from a CSV file\n",
    "\n",
    "If you are using another data source, whether that is scraped or a corpus you have found online, you may want to generate  comparable corpus using this data. For example, if you want to compare human vs generated news stories and have the human-authored stories collected, you could generate texts based on the titles of the human-authored texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856c6dd",
   "metadata": {},
   "source": [
    "Set the path to save the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example3-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7206a",
   "metadata": {},
   "source": [
    "Check if the path exists, if not create it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a13159",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_path):\n",
    "    print(f'Creating path: {corpus_path}')\n",
    "    os.makedirs(corpus_path)\n",
    "else:\n",
    "    print(f'Path already exists: {corpus_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407c9cbc",
   "metadata": {},
   "source": [
    "Specify the CSV file name here and the field name you are using for prompting. The example is from some recent RadioNZ stories about AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'sample-for-example3.csv'\n",
    "field_name = 'title'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0cce0",
   "metadata": {},
   "source": [
    "Read the data for field_name and preview it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file, 'r', newline='') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    header = csv_reader.fieldnames\n",
    "    print(f'Header fields: {header}')\n",
    "    if field_name not in header:\n",
    "        print(f'The field name {field_name} is not in the header row!')\n",
    "    else:\n",
    "        seed_data = []\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            seed_data.append(row['title'])\n",
    "            \n",
    "print(f'Data for prompting: {seed_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70655fbe",
   "metadata": {},
   "source": [
    "Now we will generate a tiny corpus of three documents based on these title field.  \n",
    "Note: we are specifying a system prompt, but there is no user prompt.  \n",
    "The prompt for each story is the title we have generated above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Write a news story for Radio New Zealand based on the title provided. This should be written for a general audience. \n",
    "Do not include the title as part of the output.\n",
    "'''\n",
    "\n",
    "max_tokens = 2048\n",
    "response_format = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa07f11",
   "metadata": {},
   "source": [
    "Run this to generate the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in seed_data:\n",
    "    print(f'Generating text based on: {prompt}')\n",
    "    response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format)\n",
    "    print(f'Text {i} preview: {response[0:200]} ...')\n",
    "\n",
    "    filename = slugify(prompt) + '.txt' # Note: this creates a nice filename from the title\n",
    "    with open(os.path.join(corpus_path, filename), 'w', encoding='utf8') as f:\n",
    "        print(f'Saving to {os.path.join(corpus_path, filename)}')\n",
    "        f.write(response)\n",
    "    \n",
    "    print('-----------------')\n",
    "    time.sleep(10) # always use a time delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac375c8",
   "metadata": {},
   "source": [
    "## Delete your corpus (if you need to)\n",
    "\n",
    "If you made a mistake or want to remove your corpus files for some other reason. Change `i_want_to_delete_my_files` from `False` to `True`. Change it back again afterwards so you don't accidentally run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5710a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'example1-corpus/' # set this to whatever path makes sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_want_to_delete_my_files = False\n",
    "\n",
    "if i_want_to_delete_my_files == True:\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(corpus_path, filename)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0615a",
   "metadata": {},
   "source": [
    "## Save a zip file of your corpus\n",
    "\n",
    "If you are happy with your corpus and want to download it, you can zip it and download the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file_name = 'example1-corpus.zip'\n",
    "corpus_path = 'example1-corpus/'\n",
    "\n",
    "shutil.make_archive(corpus_file_name[:-4], 'zip', corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105de5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
