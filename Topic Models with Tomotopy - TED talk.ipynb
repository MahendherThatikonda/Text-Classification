{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c2ff85-b251-4dd1-9c69-40af07976cc0",
   "metadata": {},
   "source": [
    "#  Topic Models with tomotopy \n",
    "With this notebook you can load a corpus from a zip file and create topic models using the Python package tomotopy. Tomotopy is a Python extension of *tomoto* (*to*pic *mo*deling *to*ol), which is a [Gibbs-sampling](https://www.youtube.com/watch?v=BaM1uiCpj_E) based topic model library written in C++. \n",
    "\n",
    "Read more in the tomotopy documentation... \n",
    "## <img src=https://bab2min.github.io/tomotopy/tomoto.png align=\"left\" width=\"20\">[**tomotopy**](https://bab2min.github.io/tomotopy/v0.12.2/en/)\n",
    "\n",
    "Work through the notebook. The key things to do are:\n",
    "1. ensure you know how to upload and unzip datafiles;\n",
    "2. understand what the model parameters are doing and try different values to see the effect they have on the topics returned;\n",
    "3. try training some different size models (e.g. 10 topics, 30 topics, 50 topics);  \n",
    "4. explore the topic for documents and assess the quality of topics returned; \n",
    "5. measure topic coherence for a number of models using the different coherence metrics;\n",
    "6. infer topics for an unseen document and find similar documents based on the topic distributions;\n",
    "7. make notes on your observations of different models and the kinds of similarities between documents they produce.\n",
    "\n",
    "Since we need to evaluate topic models against a use case - consider the idea of a recommendation engine: what model performs best for recommending similar items and why?\n",
    "\n",
    "**Important:** Each time you change settings, you will need to re-run the cells that create the topic modelling pipeline.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-machinery",
   "metadata": {},
   "source": [
    "## Import the necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860bc2b-9ac7-495a-b9a4-e9ede8de88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various packages\n",
    "from zipfile import ZipFile\n",
    "import os.path\n",
    "from os import path\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "# from collections import Counter, Iterable\n",
    "import datetime\n",
    "\n",
    "# topic modeling / nlp packages\n",
    "import tomotopy as tp\n",
    "\n",
    "# visualisation / exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "from IPython.display import IFrame, Markdown, display\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# These last lines of code suppress deprecation warnings displaying in the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-madrid",
   "metadata": {},
   "source": [
    "### Install Python packages if needed\n",
    "\n",
    "After running the cell above you may see an import error telling you that you are missing certain packages such as tomotopy or seaborn. If this is the case, uncomment the relevant packages in the cell below, run the cell, and then re-run the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac217a1-8cc8-424b-8434-41829f859ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tomotopy\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-explanation",
   "metadata": {},
   "source": [
    "**Note:** if you get an error with stopwords later in the notebook, uncomment the first two lines in the cell below and re-run... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32bb18-a0a3-4098-898b-dcab478603f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-deficit",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "\n",
    "The following cells contain functions to load a corpus from a directory of text files and preprocess the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44941f0-8049-4dd4-9f4d-3779a87a007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(path):\n",
    "    file_list = glob.glob(path + '/*.txt')\n",
    "\n",
    "    # create document list:\n",
    "    documents_list = []\n",
    "    source_list = []\n",
    "    for filename in file_list:\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            if len(text.split()) > 100: # only keep docs longer than 100 words\n",
    "                documents_list.append(text)\n",
    "                source_list.append(os.path.basename(filename))\n",
    "    print(f\"Total Number of Documents in '{path}':\",len(documents_list))\n",
    "    \n",
    "    return documents_list, source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c66c2-df0b-43bc-9f61-129eb0ec31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set, extra_stopwords={}):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    \n",
    "    # replace all newlines or multiple sequences of spaces with a standard space\n",
    "    doc_set = [re.sub(r'\\s+', ' ', doc) for doc in doc_set]\n",
    "    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # add any extra stopwords\n",
    "    if (len(extra_stopwords) > 0):\n",
    "        en_stop = en_stop.union(extra_stopwords)\n",
    "    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff3fe0-84b5-4e91-8577-b57f3a5662b4",
   "metadata": {},
   "source": [
    "## Unzip a corpus\n",
    "\n",
    "Upload the zip file containing your corpus of text files to the same directory as this notebook. The corpus_filename variable below must match the name of your zip file. Then run the following cell to unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155b83e-bb0a-425a-9440-ab0030039034",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_filename = 'transcripts2008-2018.zip' # change to the name of your dataset zip file\n",
    "\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile(corpus_filename, 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc95a-b5a4-4aa9-8df4-16fbe91317af",
   "metadata": {},
   "source": [
    "## Load and pre-process the corpus\n",
    "Load the corpus and preprocess with additional stop words (if required).\n",
    "\n",
    "The name of the directory that contains the extracted corpus text files should be assigned to the 'corpus_dirname' variable below. Make sure the only text files in this directory are those belonging to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78647a9-749d-45ee-8984-c3f6d2b6fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dirname = 'transcripts2008-2018'\n",
    "\n",
    "# adjust the path below to wherever you have the transcripts2018 folder\n",
    "documents_list, source_list = load_data_from_dir(corpus_dirname)\n",
    "\n",
    "# You can add extra stopwords here, between the curly brackets, in addition to NLTK's stopword list\n",
    "doc_clean = preprocess_data(documents_list, {'applause', 'laughter'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2a1fe-4e05-491f-8538-98753931c668",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "In the cell below you can set the parameters of the LDA topic model. More information about the tomotopy LDA model parameters can be found [here](https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel).\n",
    "\n",
    "* α – alpha, a Dirichlet prior on the per-document topic distribution\n",
    "* β – beta / eta, a Dirichlet prior on the per-topic word distribution\n",
    "* optim_interval - how often to optimise the beta hyperparameter\n",
    "* k – the number of topics in the model\n",
    "* burn-in – the number of burn-in iterations\n",
    "* iter – the number of iterations\n",
    "\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Read about the hyperparameters (settings) used in the tomotopy LDA topic model and consider the effect you expect each one to have on the results. Experiment with models using different settings. The settings will be displayed in your model summary; you can copy this into a Word doc or text file to keep a record of the settings you have tried. \n",
    "    Write a new combination of settings you tried in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72972a2",
   "metadata": {},
   "source": [
    "_Your answer here..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd953a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum frequency of words (integer)\n",
    "# Words with a smaller document frequency than min_df are excluded from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "# For more info see https://bab2min.github.io/tomotopy/v0.12.3/en/#vocabulary-controlling-using-cf-and-df\n",
    "min_doc_freq = 5\n",
    "\n",
    "# Number of top words to be removed (integer)\n",
    "# Setting this to 1 or more removes common words from the model\n",
    "# Default is 0 - i.e. no words are excluded\n",
    "remove_top = 10\n",
    "\n",
    "# Number of topics to return, between 1 and 32767\n",
    "num_topics = 20\n",
    "\n",
    "# You can read more about the following alpha and beta hyperparameters here:\n",
    "# https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "# Alpha\n",
    "# Hyperparameter of Dirichlet distribution for document-topic\n",
    "# Controls the density of topics per document\n",
    "# a float\n",
    "doc_topic = 0.1\n",
    "\n",
    "# Beta\n",
    "# Hyperparameter of Dirichlet distribution for topic-word\n",
    "# Note this is 'eta' in tomotopy - it's not a typo!\n",
    "# Controls the density of words per topic\n",
    "# a float\n",
    "topic_word = 0.01\n",
    "\n",
    "# Set the burn-in\n",
    "# Number of initial iterations that are discarded before optimising hyperparameters\n",
    "# This speeds up the convergence of the model on an optimal set of topics\n",
    "brn_in = 10\n",
    "\n",
    "# Number of iterations of the Gibbs sampler\n",
    "# If we specify 20 here, we will run 200 (10*20) iterations of Gibbs sampling total in the training loop\n",
    "num_iterations = 20\n",
    "\n",
    "# Set the top n words from the topic to display in the output of results\n",
    "num_topic_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-forwarding",
   "metadata": {},
   "source": [
    "## Train the model and display the results\n",
    "Run the cell below to train the model and display the results (you shouldn't need to change anything here).\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Look through the output and discuss with a small group of classmates. Consider the impact of the different parameter settings on the results. There are several ways to optimise and tweak the results. Try to:\n",
    "    \n",
    "- test more or fewer topics.\n",
    "- test more or fewer number of iterations\n",
    "- test the `min_doc_freq` and `remove_top` variables\n",
    "    \n",
    "Which parameter settings produce the clearest topic groupings?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3d45c",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d97b-63e2-4011-a51f-80e086333740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://bab2min.github.io/tomotopy/v0.12.2/en/\n",
    "\n",
    "# Intialize the model\n",
    "\n",
    "# The default term weighting is used\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE,\n",
    "                    min_df=min_doc_freq, \n",
    "                    rm_top=remove_top, \n",
    "                    k=num_topics, \n",
    "                    alpha=doc_topic, \n",
    "                    eta=topic_word\n",
    "                   )\n",
    "\n",
    "model.burn_in = brn_in\n",
    "\n",
    "# Add each document to the model\n",
    "for text in doc_clean:\n",
    "    model.add_doc(text)\n",
    "\n",
    "print(\"Topic Model Training...\\n\")\n",
    "\n",
    "# train the model\n",
    "# the loop reports LL/word every 10 iterations\n",
    "# this is a measure of model fit to the data (higher is better)\n",
    "for i in range(0, 100, 10):\n",
    "    model.train(iter=num_iterations)\n",
    "\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print(f'\\nTop 10 words of topic #{topic_number}\\n')\n",
    "    print(model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    print()\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    \n",
    "    \n",
    "print(\"\\nModel Summary\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the frequent words removed by rm_top. Do we want these in the model?\n",
    "model.removed_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b1684-fa7e-4765-8287-df2f616e4946",
   "metadata": {},
   "source": [
    "## Visualise the topic model\n",
    "\n",
    "The topic model can be visualised using the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/index.html) Python library, which is adapted from the R package developed by Carson Sievert and Kenny Shirley (see link to the original paper below).\n",
    "\n",
    "The left side of the pyLDAvis chart shows us the distribution of the topics in our model, and how closely they are related. The right side of the chart shows us the most relevant terms for the selected topic. Decreasing the value of lambda using the slider on the top right lets you re-rank the words displayed to be more specific to your topic of interest. This can make it easier to see what the topic is describing. Hovering over a word will display the other topics the word appears in on the left side of the chart. You can watch a video [here](https://www.youtube.com/watch?v=IksL96ls4o0) that explains how to interpret the LDAvis chart in much more detail.\n",
    "\n",
    "Read more:\n",
    "[Sievert, C., & Shirley, K. E. (2014). LDAvis: A method for visualizing and interpreting topics. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces (pp. 63-70). Baltimore: Association for Computational Linguistics.](http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)\n",
    "\n",
    "**Important note:** the pyLDAvis topic numbers will be off by one from the results in our model summary (i.e., Topic 1 in pyLDAvis will be Topic 0 from the tomotopy output above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d090ebe-682a-4e3d-af41-767b01c07186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/bab2min/tomotopy/blob/main/examples/lda_visualization.py\n",
    "# Run this cell, then look for the 'ldavis.html' file in your Jupyterhub directory - open it in a new tab\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
    "vocab = list(model.used_vocabs)\n",
    "term_frequency = model.used_vocab_freq\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
    ")\n",
    "pyLDAvis.save_html(prepared_data, 'ldavis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c36a7-ff12-4ced-99cf-10a1c1845c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the html visualisation\n",
    "# You can view it here but it's better to open the html file in a new browser tab\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "IFrame(src='ldavis.html', width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-incentive",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Take five minutes to explore the interactive visualisation and try different values of &lambda; (lambda). This allows you examine words in topics along a scale, weighted entirely by the probability of the word given the topic if &lambda; = 1, or weighted by a ratio of this to the probability (ie frequency) of the word in the whole corpus. What value of &lambda; filters the topics best? Are there good / bad topics here? Write down some examples.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2afbf4",
   "metadata": {},
   "source": [
    "_Your answer here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bf53d-176a-462b-a756-b1db0217b782",
   "metadata": {},
   "source": [
    "## Examine top documents for a given topic\n",
    "The following code to display the top documents is adapated from [***Topic Modeling - With Tomotopy***](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html) from the book *Introduction to Cultural Analytics & Python* by Melanie Walsh (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca5faf-d7bd-487e-a716-fe76434bfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "\n",
    "topic_indiv_150_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words_150 = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=150))\n",
    "    topic_indiv_150_words.append(topic_words_150.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974316dc-6ffb-401c-be8c-3a3cf0e94302",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def make_md(string):\n",
    "    display(Markdown(str(string)))\n",
    "\n",
    "def get_top_docs(docs, sources, topic_indiv_150_words, topic_distributions, topic_index, n):\n",
    "    \n",
    "    sorted_data = sorted([(_distribution[topic_index], _document, _sources) \n",
    "                          for _distribution, _document, _sources\n",
    "                          in zip(topic_distributions, docs, sources)], reverse=True)\n",
    "    \n",
    "    top_25 = \", \".join(topic_indiv_150_words[topic_index][:25])\n",
    "    make_md\\\n",
    "    (f\"### Topic {topic_index}\\n\\\n",
    "    \\n{top_25} ...\\\n",
    "    \\n\\n**Note**: words from the top 150 topic words are shown in bold in the text\\n\\n---\")\n",
    "    \n",
    "    for probability, doc, source in sorted_data[:n]:\n",
    "        # Make topic words bolded\n",
    "        for word in topic_indiv_150_words[topic_index]:\n",
    "            doc = doc.lower()\n",
    "            if word in doc:\n",
    "                doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc)\n",
    "                #doc = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", doc, re.IGNORECASE)\n",
    "        \n",
    "        make_md(f'  \\n**Topic Probability**: {probability}  \\n**Source**: {source}  \\n**Document**: {doc}  \\n\\n---')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04baae-6fc2-4f76-8e29-dbab4ac786c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the topic number to explore\n",
    "topic_no = 3\n",
    "\n",
    "# Set the number of 'top docs' to display\n",
    "num_top_docs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7273b-32ac-4de4-b886-4ff4005ff29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top documents for the selected topic, with topic words highlighted\n",
    "\n",
    "get_top_docs(documents_list, \n",
    "             source_list,\n",
    "             topic_indiv_150_words, \n",
    "             topic_distributions, \n",
    "             topic_index = topic_no, \n",
    "             n = num_top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aa79a",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> Choose a topic and write a couple of sentences below that describe what the topic is about, noting words that clearly fit together and any odd words that do not seem to fit.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212a16a",
   "metadata": {},
   "source": [
    "_Your description of a topic here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-rally",
   "metadata": {},
   "source": [
    "## Infer topics for an unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to unzip a folder of new transcripts\n",
    "unseen_corpus_filename = 'ted-transcripts2019.zip' # change to the name of your dataset zip file\n",
    "\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile(unseen_corpus_filename, 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall(path='transcripts2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the document for which you want to infer the topics\n",
    "new_doc_path = 'transcripts2019/transcripts2019/2019-01-22-iseult_gillespie_why_should_you_read_fahrenheit_451.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the new document and add it to the model, ready to infer topics\n",
    "with open(new_doc_path, 'r', encoding='utf8') as f:\n",
    "    new_text = f.read()\n",
    "\n",
    "new_text_clean = preprocess_data([new_text], {'applause', 'laughter'})\n",
    "new_doc = model.make_doc(new_text_clean[0])\n",
    "\n",
    "print(new_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer topics in the new doc\n",
    "\n",
    "new_doc_topics = model.infer(new_doc)\n",
    "\n",
    "new_doc_list = []\n",
    "\n",
    "for topic, prop in enumerate(new_doc_topics[0].tolist()):\n",
    "    new_doc_list.append((topic,prop))\n",
    "    \n",
    "new_doc_topics_sorted = sorted(new_doc_list, key=lambda x: x[1], reverse=True) # sorts document topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display topic proportions and words for the new document\n",
    "\n",
    "for topic, prop in new_doc_topics_sorted:\n",
    "    topic_words = [word for word, prop in model.get_topic_words(topic_id=topic, top_n=num_topic_words)]\n",
    "    print(\"%.3f\" % prop, topic, topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-japan",
   "metadata": {},
   "source": [
    "## Find similar documents\n",
    "\n",
    "We can documents with a similar topic distribution using Jensen-Shannon distance. A lower distance score indicates documents have more similar topics.\n",
    "\n",
    "For more information, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "jenshan_scores = []\n",
    "\n",
    "# here topic_distributions from the 'Examine top documents' section are used\n",
    "for doc, distribution in enumerate(topic_distributions):\n",
    "    a = new_doc_topics[0].tolist()\n",
    "    b = distribution\n",
    "    jenshan_score = distance.jensenshannon(a, b)\n",
    "    jenshan_scores.append((doc,jenshan_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_index = sorted(jenshan_scores, key=lambda x: x[1]) # The smaller the Jensen-Shannon distance, the more similar the docs\n",
    "\n",
    "print('Similar documents by Jensen-Shannon distance (lower is better):\\n')\n",
    "\n",
    "for i in range(3): \n",
    "    document_id, js_score = dist_index[i]\n",
    "    print('---------',document_id, js_score,'---------')\n",
    "    print(re.sub('\\s+', ' ', documents_list[document_id][:2000]))\n",
    "    print()\n",
    "\n",
    "    doc_list = []\n",
    "    for i, text in enumerate(documents_list):\n",
    "        doc_list.append((i, text))\n",
    "    \n",
    "    document_topics = enumerate(topic_distributions[document_id]) \n",
    "    document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) # sorts document topics\n",
    "    \n",
    "    for topic, prop in document_topics:\n",
    "        topic_words = [word for word, prop in model.get_topic_words(topic_id=topic, top_n=num_topic_words)]\n",
    "        if prop > 0.05:\n",
    "            print (\"%.3f\" % prop, topic, topic_words)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229822be-fcab-478e-98d0-1783597c982d",
   "metadata": {},
   "source": [
    "## Get a coherence score\n",
    "Topic coherence scores attempt to measure how similar the prominent words in a topic are to each other and, as a result, how easy the topic is for a human to interpret. Some of the better topic coherence metrics are based on mutual information, a collocation measure we've seen already. You don't need to study these in detail for this course, but if you're curious can read more about them in [Roder et al (2015)](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf), [Lau et al (2014)](https://aclanthology.org/E14-1056.pdf) and nice summary [here](https://palmetto.demos.dice-research.org/) that explains the c_uci score: \n",
    "\n",
    ">\"c_uci is a coherence that is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words. The word cooccurrence counts are derived using a sliding window with the size 10. For every word pair the PMI is calculated. The arithmetic mean of the PMI values is the result of this coherence. c_npmi is an enhanced version of the c_uci coherence using the normalized pointwise mutual information (NPMI) instead of the pointwise mutual information (PMI).\"\n",
    "\n",
    "The code below uses the tomotopy presets to calculate the average coherence for the model and the coherence per topic. There are various coherence metrics available:\n",
    "* u_mass\n",
    "* c_uci\n",
    "* c_npmi (we'll use this as an example. It builds on c_uci.)\n",
    "* c_v (not recommended anymore - the authors have stated there's a problem with it)\n",
    "* c_p (not available through tomotopy, but through the palmetto library or webservice). \n",
    "\n",
    "These scores can be used to assess whether changes in the number of topics or other hyperparameters improves the model when you have large numbers of topics. However, it is very important to also use qualitative assessment, since in many cases coherence scores are not as reliable as human judgment.\n",
    "\n",
    "The next code block calculates and prints a c_npmi coherence score for each topic in our current model, as well as an average score for the model as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652538d-abb8-4a06-8341-4e2bb4ca3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherence scores for each topic\n",
    "coh = tp.coherence.Coherence(model, coherence='c_npmi')\n",
    "average_coherence = coh.get_score()\n",
    "coherence_per_topic = [coh.get_score(topic_id=k) for k in range(model.k)]\n",
    "\n",
    "# Print the average coherence score\n",
    "print('==== Coherence : c_npmi ====')\n",
    "print(f'Average: {average_coherence:.6f}')\n",
    "\n",
    "# Print the topic number along with its coherence score\n",
    "print('Per Topic:')\n",
    "for topic_idx, coherence_score in enumerate(coherence_per_topic):\n",
    "    print(f'Topic {topic_idx}: {coherence_score:.6f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba4bd4",
   "metadata": {},
   "source": [
    "To visualise the variation in the per-topic coherence scores we could use a box & whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_per_topic_df = pd.DataFrame(coherence_per_topic)\n",
    "coh_per_topic_df.plot.box(title='c_npmi coherence scores for our ' + str(model.k) + ' topics in this model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad282d-267a-40da-b5f5-f0952225d203",
   "metadata": {},
   "source": [
    "## Calculate coherence scores for models with different numbers of topics\n",
    "\n",
    "The code below creates further models below with different numbers of topics and return the **c_npmi** coherence score for each of them. The other parameters remain as you set them at the start of the notebook.\n",
    "\n",
    "NPMI scores will range from 1 to -1. For a single pair of topic words the score can be interpreted as follows:\n",
    "\n",
    "* 1 = the words only co-occur in a sliding window together, never independently (positive collocation)\n",
    "* 0 = the words co-occur with the same probability as the product of their independent probabilities (null / no collocation)\n",
    "* -1 = the words never co-occur in a sliding window together, and only appear independently (negative collocation, more likely not to appear together than expected!)\n",
    "\n",
    "Higher scores indicate greater coherence for this measure. We generally want to compare two or more scores for different topics or models.\n",
    "\n",
    "If you want a bit more detail, you can read about how normalise pointwise mutual information is calculated in section 3.1 of [Bouma 2009](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf) who is Lau et al's source for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49f755-82d3-4e96-8f7b-36d040e9095e",
   "metadata": {},
   "source": [
    "## Test a range of topic sizes and plot the results\n",
    "**Important**: training a lot of models with larger corpora could take a long time. Experiment with small numbers of models before going large, so as not to be waiting too long! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e1fcd-5e61-4a91-ae93-b75bea9dd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply values for k and the interval\n",
    "# eg with min_k=5, max_k=100, and intervals=5 will train models with 5, 10, 15, 20 ... 100 topics.\n",
    "min_k = 5\n",
    "max_k = 50\n",
    "intervals = 5\n",
    "\n",
    "coherences = {}\n",
    "\n",
    "for i in range(min_k, max_k + 1, intervals):   \n",
    "    lda_mod = tp.LDAModel(tw=tp.TermWeight.ONE,\n",
    "                        min_df = min_doc_freq, \n",
    "                        rm_top = remove_top, \n",
    "                        k = i, \n",
    "                        alpha = doc_topic, \n",
    "                        eta = topic_word\n",
    "                       )\n",
    "\n",
    "    lda_mod.burn_in = brn_in\n",
    "\n",
    "    # Add each document to the model\n",
    "    for text in doc_clean:\n",
    "        lda_mod.add_doc(text)\n",
    "\n",
    "    lda_mod.train(iter=num_iterations)\n",
    "\n",
    "    coherences[i] = tp.coherence.Coherence(lda_mod, coherence = 'c_npmi').get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf3589-28fe-4f94-bb0b-ead555b08598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the coherence scores to a pandas dataframe\n",
    "df = pd.DataFrame.from_dict(coherences, orient='index', columns=['Coherence'])\n",
    "df['Topics'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2715475-fe4b-49d6-8c64-ed9ae4fa6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "# plot the result\n",
    "df.plot(kind='line', x='Topics', y='Coherence', ylim=(0,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-tours",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> What number of topics produces the best coherence score? How does this compare with your qualitative assessment of topic quality? Write about your findings below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a77bc7",
   "metadata": {},
   "source": [
    "_Your findings here..._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
