{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51655f1c-23d9-45d0-9dd9-16afa391f778",
   "metadata": {},
   "source": [
    "#  Genre classification of historical newspaper texts\n",
    "\n",
    "In this notebook we will train and test two different classifiers on a collection of texts from [historical New Zealand newspapers](https://paperspast.natlib.govt.nz/newspapers). Our aim is to build genre classification models that are independent of topic, so we will use features based on the structure and layout of the text (for example line widths), linguistic features (such as the frequency of certain parts-of-speech), and other text statistics.\n",
    "\n",
    "The data used in this notebook is originally sourced from the [National Library of New Zealand's Papers Past open data](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/dataset-papers-past-newspaper-open-data-pilot). It consists of a small dataset of articles that have been pre-labelled with their genre and includes features related to line widths and offsets that have been extracted from the [METS/ALTO XML files](https://veridiansoftware.com/knowledge-base/metsalto/) for each newspaper.\n",
    "\n",
    "We will use [spaCy](https://spacy.io/) and [textstat](https://pypi.org/project/textstat/) to extract additional features and add them to our dataframe. We will then use [scikit-learn](https://scikit-learn.org/stable/) to train and test our models.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>\n",
    "\n",
    "![National Library Papers Past](https://images.ctfassets.net/pwv49hug9jad/6tW2XbQ3rwBfOYilgpZmVQ/468368a1454e2201958401cab2ea7d79/guides-pp-open-data-feature-image.jpg?fm=webp)\n",
    "\n",
    "[Image source: natlib.govt.nz](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/get-started-papers-past-newspaper-open-data-pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c691409-29ef-40dd-9a0c-b6352d6418fb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-disclosure",
   "metadata": {},
   "source": [
    "We need to make sure the libraries we will need in this notebook are installed (you only need to run this cell once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn --upgrade\n",
    "!pip install textstat\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-volleyball",
   "metadata": {},
   "source": [
    "Now import the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d297294-06bf-41bf-a157-dd55226de21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Classifier training and evaluation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Feature extraction\n",
    "import spacy\n",
    "import textstat\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"whitegrid\",\n",
    "    font=\"sans-serif\",\n",
    "    font_scale=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb30ec9-266f-4df9-acd6-8d0d2c78e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.cli.download(\"en_core_web_sm\")  # uncomment if needed\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e1848-5064-46dc-b8fb-a90e41ead2c5",
   "metadata": {},
   "source": [
    "## Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb895d2-84af-4b02-9039-408685be95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "filepath = \"paperspast_4genres_20240502.csv\"\n",
    "df = pd.read_csv(filepath, index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0188bb-112f-40c4-a27c-d4778964dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the count of articles by genre\n",
    "display(df.groupby([\"genre\"])[\"genre\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554e5d4-b0e2-4165-9366-8394726b315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first ten rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864422c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the ID of the newspaper 'Northern Advocate' is 'NA', this has been read-in as NaN\n",
    "# View the problem by selecting rows where 'newspaper' column equals 'Northern Advocate'\n",
    "# Look at the 'newspaper_id' column\n",
    "\n",
    "mask = df[\"newspaper\"] == \"Northern Advocate\"\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fix this by filling the newspaper_id column for our selected rows with the correct code 'NA'\n",
    "df.loc[mask, \"newspaper_id\"] = df.loc[mask, \"newspaper_id\"].fillna(\"NA\")\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the articles in our dataset by newspaper\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "sample_papers_unique = df[\"newspaper\"].nunique()\n",
    "print(\"-----------------------------------------------------\") \n",
    "print(f\"Number of newspaper titles in sample dataset: {sample_papers_unique}\") \n",
    "print(\"-----------------------------------------------------\") \n",
    "print(\"\") \n",
    "\n",
    "ax_1 = sns.countplot(x = \"newspaper\", \n",
    "                     data = df, \n",
    "                     order = df[\"newspaper\"].value_counts().index, \n",
    "                     color = \"#32a5fc\")\n",
    "ax_1.set_xlabel(\"Newspaper\", fontsize = 12)\n",
    "ax_1.set_ylabel(\"Count of articles\", fontsize = 12, labelpad = 9)\n",
    "ax_1.set_title(\"Distribution of articles by newspaper\", fontsize = 14)\n",
    "sns.despine(top = True, right = True, left = True, bottom = False, offset = None, trim = False)\n",
    "plt.xticks(rotation = 90, fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's view the distribution of the articles in our dataset by year\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "annual_df = (df.groupby(df[\"year\"])\n",
    "             [\"text\"].count().reset_index())\n",
    "ax_2 = sns.barplot(x = \"year\", \n",
    "                   y = \"text\", \n",
    "                   data = annual_df, \n",
    "                   color = \"#32a5fc\")\n",
    "ax_2.set_xlabel(\"Year\", fontsize = 12, labelpad = 14)\n",
    "ax_2.set_ylabel(\"Count of articles\", fontsize = 12, labelpad = 9)\n",
    "ax_2.set_title(\"Distribution of articles in dataset by year\", fontsize = 14)\n",
    "sns.despine(top = True, right = True, left = True, bottom = False, offset = None, trim = False)\n",
    "plt.xticks(rotation = 90, fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583aef-080f-427a-b883-6a7c7f633dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the full text of a selected article by dataframe index\n",
    "selected_index = 431\n",
    "\n",
    "print(f\"\\nGenre: {df['genre'].values[selected_index]}\\n\")\n",
    "print(\"==============\\n\")\n",
    "print(f\"Title:\\t\\t{df['title'].values[selected_index]}\")\n",
    "print(f\"Newspaper:\\t{df['newspaper'].values[selected_index]}\")\n",
    "print(f\"Date:\\t\\t{df['day'].values[selected_index]} / {df['month'].values[selected_index]} / { df['year'].values[selected_index]}\\n\")\n",
    "print(f\"{df['text'].values[selected_index]}\\n\")\n",
    "print(f\"You can view the scanned article on the Papers Past website. \"\n",
    "      f\"Follow the link below to see the scan of the original article.\\n{df['article_web'].values[selected_index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe938d-098c-43a0-a6d2-525b66516574",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "You'll see from the above that there can be symbols and punctuation in the text that are the result of [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) errors. We will run a simple cleaner function over the text column of the dataframe to improve this and add the cleaned text to a new column. Before we remove punctuation, we will count the sentences and add this feature to the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10d6d-72cf-4383-b103-a1441004b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Given a dataframe column of OCR text, count the sentences, question marks, quotation marks,\n",
    "    exclamation marks, and apostrophes and store these counts in new columns.\n",
    "    \n",
    "    Remove symbols using a regular expression and create a clean text column.\n",
    "\n",
    "    Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df[\"sentence_count\"] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Count occurrences of specific characters and add to new columns\n",
    "    df[\"freq_q_marks\"] = df[column_name].apply(lambda x: x.count(\"?\"))\n",
    "    df[\"freq_double_quotes\"] = df[column_name].apply(lambda x: x.count('\"'))\n",
    "    df[\"freq_exclam\"] = df[column_name].apply(lambda x: x.count(\"!\"))\n",
    "    df[\"freq_apost\"] = df[column_name].apply(lambda x: x.count(\"'\"))\n",
    "\n",
    "    # Regex pattern for only alphanumeric text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9]{1,50}\")\n",
    "    df[\"clean_text\"] = df[column_name].str.findall(pattern).str.join(\" \")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a4177-4db9-4c31-aa96-f63252ea5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaner(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at that same text after 'cleaning'\n",
    "\n",
    "# We can display the full text of a selected article by index\n",
    "print(f\"\\nGenre: {df['genre'].values[selected_index]}\\n\")\n",
    "print(\"==============\\n\")\n",
    "print(f\"Title:\\t\\t{df['title'].values[selected_index]}\")\n",
    "print(f\"Newspaper:\\t{df['newspaper'].values[selected_index]}\")\n",
    "print(f\"Date:\\t\\t{df['day'].values[selected_index]} / {df['month'].values[selected_index]} / {df['year'].values[selected_index]}\\n\")\n",
    "print(df[\"clean_text\"].values[selected_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-values",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> We have done a very simple clean-up of the text but, as you can see, there are still problems. You might see incorrect words like 'np' instead of 'up', capital letters in the wrong place, numbers where there should be letters, and more. Think about the impact this might have on our model and discuss it with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bbfbb-465d-4c63-9d76-5ac2ed780d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see our additional columns have been added to the end of our dataframe\n",
    "# Scroll across to the right if they are not visible\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574a183-116d-46cf-ac2d-5aaab1b95b82",
   "metadata": {},
   "source": [
    "## Feature extraction: linguistic features and text statistics\n",
    "\n",
    "The following cells extract parts-of-speech and text statistic features and add them to the dataframe. For efficiency, the texts are [processed for parts-of-speech tagging](https://spacy.io/usage/processing-pipelines) as a stream using spaCy's [nlp.pipe](https://spacy.io/usage/processing-pipelines#processing). This allows the texts to be buffered in batches instead of one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058fe54-7617-47b2-9bb5-bbc9caf4f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define the list of POS tags to count (you don't need to change anything here)\n",
    "# We will use a selection of Universal POS tags: https://universaldependencies.org/u/pos/ \n",
    "\n",
    "pos_tags = [\n",
    "            \"ADJ\",    # adjective\n",
    "            \"ADV\",    # adverb\n",
    "            \"NOUN\",   # noun\n",
    "            \"NUM\",    # numeral\n",
    "            \"PRON\",   # pronoun\n",
    "            \"PROPN\",  # proper noun\n",
    "            \"VERB\",   # verb\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797ee69-8406-4d0b-a157-a85e7dbbc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(df, pos_tags):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe with a column called \"clean_text\"\n",
    "    and a list of Universal parts of speech tags, add columns for \n",
    "    a list of tokens, word count, relative frequencies \n",
    "    of the given parts of speech (using Spacy), relative frequencies \n",
    "    of stopwords, and relative frequencies of monosyllabic words \n",
    "    and selected punctuation. \n",
    "\n",
    "    Return the dataframe with the additional columns.\n",
    "    \"\"\"\n",
    "    stop = stopwords.words(\"english\")\n",
    " \n",
    "    token_list = []\n",
    "    pos_counts = []\n",
    "    input_col = \"clean_text\"\n",
    "\n",
    "    # Spacy pipeline to count POS\n",
    "    nlp_text_pipe = nlp.pipe(df[input_col], batch_size = 20)\n",
    "\n",
    "    for doc in nlp_text_pipe:\n",
    "        token_list.append([token.text for token in doc if not token.is_punct and not token.is_space]) \n",
    "        pos_counts.append(Counter(token.pos_ for token in doc if token.pos_ in pos_tags))\n",
    "    \n",
    "    df[\"tokens\"] = token_list\n",
    "    df[\"word_count\"] = df[\"tokens\"].apply(lambda x: len(x))\n",
    "    df[\"stopwords_count\"] = df[input_col].apply(lambda x: len([i for i in x.split() if i.lower() in stop]))\n",
    "    df[\"stopword_relfreq\"] = df[\"stopwords_count\"] / df[\"word_count\"]\n",
    "\n",
    "    pos_columns = set().union(*pos_counts)\n",
    "\n",
    "    # Compute the relative frequencies of each part-of-speech tag\n",
    "    for pos in pos_columns:\n",
    "        df[pos + \"_relfreq\"] = [count.get(pos, 0) for count in pos_counts] / df[\"word_count\"]\n",
    "\n",
    "    # Add monsyllabic words relative frequency using the textstat library\n",
    "    # Add relative frequencies of the punctuation marks counted earlier\n",
    "    df[\"monosyll_count\"] = df[input_col].apply(lambda x: textstat.monosyllabcount(x)) \n",
    "    df[\"monosyll_relfreq\"] = df[\"monosyll_count\"] / df[\"word_count\"]\n",
    "    df[\"q_marks_relfreq\"] = df[\"freq_q_marks\"]  / df[\"word_count\"]\n",
    "    df[\"double_quotes_relfreq\"] = df[\"freq_double_quotes\"] / df[\"word_count\"]\n",
    "    df[\"exclam_relfreq\"] = df[\"freq_exclam\"]  / df[\"word_count\"]\n",
    "    df[\"apost_relfreq\"] = df[\"freq_apost\"] / df[\"word_count\"]\n",
    "\n",
    "    # Drop count columns that are no longer required (they've been converted to relative frequencies)\n",
    "    df.drop(columns=[\"tokens\", \n",
    "                     \"monosyll_count\", \n",
    "                     \"stopwords_count\", \n",
    "                     \"freq_q_marks\", \n",
    "                     \"freq_double_quotes\", \n",
    "                     \"freq_exclam\", \n",
    "                     \"freq_apost\"], axis = 1, inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62f4f9-4772-44f6-a9e6-b38dfbb71e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to extract text features and add them to the dataframe\n",
    "# This might take a little while to run\n",
    "df = process_text(df, pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa9212-9ade-4b1a-8952-711f629290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few rows of the dataframe to see the features that have been added\n",
    "# Scroll to the right\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caab7b-7f86-436b-ba88-eca61579f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine use of a selected POS for a given dataframe index\n",
    "\n",
    "pos_var = 'PRON'\n",
    "my_ind = 4\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "doc = nlp(df.iloc[my_ind][\"clean_text\"])\n",
    "my_text = df.iloc[my_ind][\"clean_text\"]\n",
    "stop = stopwords.words(\"english\")\n",
    "my_pos = [token.text for token in doc if token.pos_ == pos_var]\n",
    "my_stopwords = [text for text in my_text.split() if text.lower() in stop]\n",
    "\n",
    "print(f\"------------------\\nIndex: {my_ind}\\n------------------\")\n",
    "display(df.loc[[my_ind]])\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"Word count: {df.iloc[my_ind]['word_count']}\")\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"{pos_var} relative frequency: {df.iloc[my_ind][f'{pos_var}_relfreq']:.3f}\")\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"Article title:\\t\\t\\t{df.iloc[my_ind]['title']}\")\n",
    "print(f\"Scanned newspaper issue:\\t{df.iloc[my_ind]['newspaper_web']}\\n\")\n",
    "print(f\"------------------\\nClean text:\\n\")\n",
    "print(df.iloc[my_ind][f\"clean_text\"])\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"{pos_var} (count = {len(my_pos)}):\\n\")\n",
    "print(my_pos)\n",
    "print(\"\\n------------------\\n\"\n",
    "        f\"Stopwords (count = {len(my_stopwords)}):\\n\")\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also inspect summary statistics for all our numerical data \n",
    "# We will use these later to explore the misclassified articles\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-pound",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Why is exploratory data anlaysis (EDA) using techniques such as visualising the data and examining descriptive statistics important? What can it reveal? Discuss with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64148-818c-4c2f-98ee-8a5f2a0a0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the full list of columns in our dataframe, and see their data types\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724c5f6-f6aa-4e4a-91d1-13169e5edd6f",
   "metadata": {},
   "source": [
    "## Specify features to include in the model\n",
    "\n",
    "* We now need to specify the features we want to include in our model, for example if we know that two features are highly correlated, we can choose to only include one in the model\n",
    "* You can include or remove features from the model to explore the impact of different combinations of features on the performance of the classifier.\n",
    "* **Use the default list shown below first, then experiment to see what effect the changes have on the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ffc25-d663-4cc0-bc37-3ff6e377e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include in the model \n",
    "# Place cursor in the text and press Ctrl + / to comment or uncomment the line\n",
    "\n",
    "features = [\n",
    "            \"avg_line_width\",\n",
    "            # \"min_line_width\",\n",
    "            # \"max_line_width\",\n",
    "            # \"line_width_range\",\n",
    "            \"avg_line_offset\",\n",
    "            # \"max_line_offset\",\n",
    "            # \"min_line_offset\",\n",
    "            # \"sentence_count\",\n",
    "            \"word_count\",\n",
    "            # \"stopword_relfreq\",\n",
    "            \"VERB_relfreq\",\n",
    "            \"ADV_relfreq\",\n",
    "            \"PRON_relfreq\",\n",
    "            \"ADJ_relfreq\",\n",
    "            \"PROPN_relfreq\",\n",
    "            \"NOUN_relfreq\",\n",
    "            \"NUM_relfreq\",\n",
    "            \"monosyll_relfreq\",\n",
    "            # \"q_marks_relfreq\",\n",
    "            # \"double_quotes_relfreq\",\n",
    "            # \"exclam_relfreq\",\n",
    "            # \"apost_relfreq\",\n",
    "    \n",
    "            # We will code our target genre as a binary class '1' and the other genres as '0'\n",
    "            # Do not remove this feature from the set\n",
    "            \"binary_class\"  \n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46cd83-e31d-4f1c-8673-890ee3649a9b",
   "metadata": {},
   "source": [
    "## Set the target genre\n",
    "\n",
    "* We will specify the genre we want to predict with the binary classifier. \n",
    "* The selected genre will be labelled as 1 in the binary classification model, with the other classes labelled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a85462-bb01-4cee-8246-d40db029a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from:\n",
    "# FamilyNotice     \n",
    "# Fiction          \n",
    "# LetterToEditor    \n",
    "# Poetry         \n",
    "\n",
    "target_genre = \"Poetry\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-peninsula",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Train and test classifiers for each of the four genres and take note of the results in a separate document. Which combination of genre and classifier achieved the best metrics and which was the worst? Discuss the results with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c3a6-3533-4464-bdfa-991f38b90ee5",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets\n",
    "\n",
    "* Run the cells below to split the data into train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34dd20-47d5-4d2e-890d-41b5d299c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(df, features, target_genre):\n",
    "    \"\"\"\n",
    "    Given the dataframe, features to include in the model,\n",
    "    and the target genre, split the data into \n",
    "    training and test sets and use the dataframe indices to \n",
    "    save the order of the split\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"binary_class\"] = np.where(df[\"genre\"]== target_genre, 1, 0)\n",
    "    model_df = df.filter(features, axis=1)\n",
    "    indices = df.index.values\n",
    "\n",
    "    # Extract the explanatory variables in X and the target variable in y\n",
    "    y = model_df.binary_class.copy()\n",
    "    X = model_df.drop([\"binary_class\"], axis=1)\n",
    "\n",
    "    # Train test split \n",
    "    # Use the indices to save the order of the split.\n",
    "    # https://stackoverflow.com/questions/48947194/add-randomforestclassifier-predict-proba-results-to-original-dataframe\n",
    "    X_train, X_test, indices_train, indices_test = train_test_split(X, \n",
    "                                                                    indices, \n",
    "                                                                    test_size = .3,    # This value changes the proportion of data held out for the test set\n",
    "                                                                    random_state = 7)  # You can change the random state to change the allocation of docs to the training and test sets\n",
    "    \n",
    "    y_train, y_test = y[indices_train], y[indices_test]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, indices_train, indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e1958-eaf9-43d1-8c7c-a8c5c3ea5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_data(df, features, target_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c5d5c-a77d-41a2-ad85-50a3bb574951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139306fc-894f-453d-9b04-ab5417bd26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195a2c3-d6b1-4e3c-b612-012c519d3c25",
   "metadata": {},
   "source": [
    "## Train and Test a Logistic Regression Classifier \n",
    "* [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) is a binary classification method popular for its computational efficiency and interpretability.\n",
    "* Run the cells below to train and test a logistic regression classifier for our selected genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e518c7-006e-4c8f-abac-a7f978c091f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_binary(X_train, X_test, y_train, y_test, target_genre):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([(\"scl\", StandardScaler()),\n",
    "                     (\"clf\", LogisticRegression())]) \n",
    "    pipe.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    y_prob_train = pipe.predict_proba(X_train)\n",
    "    y_prob_test = pipe.predict_proba(X_test)\n",
    "        \n",
    "    accuracy_result = accuracy_score(y_test, y_pred_test)\n",
    "    precision_result = precision_score(y_test, y_pred_test)\n",
    "    recall_result = recall_score(y_test, y_pred_test)\n",
    "    f1_result = f1_score(y_test, y_pred_test)\n",
    "    auroc_result = roc_auc_score(y_test, y_prob_test[:, 1])  # Use probabilities for AUROC\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Logistic Regression\")\n",
    "    print(f\"Target genre: {target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "    print(f\"Accuracy = {accuracy_result:.3f}\")\n",
    "    print(f\"Precision = {precision_result:.3f}\")\n",
    "    print(f\"Recall = {recall_result:.3f}\")\n",
    "    print(f\"F1 Score = {f1_result:.3f}\")\n",
    "    print(f\"AUROC Score = {auroc_result:.3f}\")\n",
    "    \n",
    "    RocCurveDisplay.from_predictions(y_test, y_prob_test[:, 1])  # Use probabilities for ROC curve\n",
    "    plt.title(\"AUROC: Logistic Regression\")\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Model coefficients \\nwith log odds (logit) converted to odds ratio\\nfor improved interpretability\\n\")\n",
    "    print(f\"Target genre: {target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    # Get coefficients (log odds or logit)\n",
    "    log_odds = pipe.named_steps[\"clf\"].coef_[0]\n",
    "    # Convert log odds to odds ratio\n",
    "    odds = np.exp(log_odds)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test, log_odds, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68553bc-821a-45be-a43d-8f16bb863b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_lr(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test, log_odds, odds = log_reg_binary(X_train, \n",
    "                                                                                          X_test,\n",
    "                                                                                          y_train,\n",
    "                                                                                          y_test,\n",
    "                                                                                          target_genre\n",
    "                                                                                         )\n",
    "\n",
    "    # # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,\"pred_train\"] = y_pred_train\n",
    "    df_new.loc[indices_test,\"pred_test\"] = y_pred_test\n",
    "    df_new.loc[indices_train,\"prob_0_train\"] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,\"prob_0_test\"] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,\"prob_1_train\"] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,\"prob_1_test\"] = y_prob_test[:,1]   \n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    # Create a dataframe with both log odds and odds\n",
    "    lr_odds_df = pd.DataFrame({\n",
    "        \"feature\": X_train.columns,\n",
    "        \"log odds (logit)\": log_odds,\n",
    "        \"odds ratio\": odds\n",
    "    })\n",
    "    \n",
    "    # Sort the dataframe by odds in descending order\n",
    "    lr_odds_df = lr_odds_df.sort_values(by=\"odds ratio\", ascending=False)\n",
    "    \n",
    "    # Reset the index for cleaner display\n",
    "    lr_odds_df = lr_odds_df.reset_index(drop=True)\n",
    "       \n",
    "    return df_new, lr_odds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fe472-694a-4fa9-861c-877eb7cb9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_df, lr_odds_df = genres_binary_lr(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test)\n",
    "\n",
    "# Explore the model coefficients\n",
    "display(lr_odds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b618-e940-44ef-8a9e-521fda5a50a6",
   "metadata": {},
   "source": [
    "### Interpreting the logistic regression model\n",
    "A benefit of logistic regression is that it is relatively easy to interpret compared to other classifiers. We can extract the coefficients of the features in the final model (using the 'coef_' attribute) to see which features were the strongest predictors of the positive class (in our case, the selected genre). \n",
    "\n",
    "The coefficients extracted using 'coef_' are the log odds (logit) that an observation belongs to the positive class. In order to interpret them more easily, we can convert them to the odds ratio. An odds ratio greater than 1 represents a positive association and can be interpreted as follows:\n",
    "\n",
    "**\"For every unit increase in {feature}, the odds that the observation is {positive class} are {odds ratio} times greater than the odds that it is not {positive class} when all other variables are held constant.\"**\n",
    "\n",
    "An odds ratio less than 1 represents a negative association. To describe them in a similar way to the above, we need to take 1/odds ratio. For example:\n",
    "\n",
    "\"For every unit increase in {feature}, the odds that the observation **is not** {positive class} are {1 / odds ratio} times greater than the odds that it **is** {positive class} when all other variables are held constant.\"\n",
    "\n",
    "**When interpreting the model coefficients it is important to consider the influence of features that may be correlated with each other (multicollinearity). These features will have similar predictive relationships to the outcome and therefore the sign and value of the coefficients should be interpreted with caution.** \n",
    "\n",
    "You can read more about calculating and interpreting the coefficients of regression models in this [Towards Data Science](https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1) article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e66ee-a857-47e7-b128-2a4fa6555502",
   "metadata": {},
   "source": [
    "## Train and Test a Decision Tree classifier \n",
    "* [Decision Tree](https://scikit-learn.org/stable/modules/tree.html) methods are useful because they are very easy to apply and interpret, however, the results can be susceptible to small changes in the dataset and they don't work so well for imbalanced datasets (is this a problem with our dataset?).\n",
    "* Run the cells below to train and test a Decision Tree classifier for our selected genre and compare the results to the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b044212-1a92-4a46-b872-d476424594fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_binary(X_train, X_test, y_train, y_test, target_genre, features):\n",
    "    \"\"\"\n",
    "    Train a decision tree to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([(\"clf\", \n",
    "                      DecisionTreeClassifier(random_state=343, \n",
    "                                             max_depth = 3 # Limiting the depth of the tree can help to prevent overfitting\n",
    "                                            )\n",
    "                     )]) \n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    y_prob_train = pipe.predict_proba(X_train) \n",
    "    y_prob_test = pipe.predict_proba(X_test) \n",
    "        \n",
    "    accuracy_result = accuracy_score(y_test, y_pred_test)\n",
    "    precision_result = precision_score(y_test, y_pred_test)\n",
    "    recall_result = recall_score(y_test, y_pred_test)\n",
    "    f1_result = f1_score(y_test, y_pred_test)\n",
    "    auroc_result = roc_auc_score(y_test, y_prob_test[:, 1])  # Use probabilities for AUROC\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Decision Tree\")\n",
    "    print(f\"{target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "    print(f\"Accuracy = {accuracy_result:.3f}\")\n",
    "    print(f\"Precision = {precision_result:.3f}\")\n",
    "    print(f\"Recall = {recall_result:.3f}\")\n",
    "    print(f\"F1 Score = {f1_result:.3f}\")\n",
    "    print(f\"AUROC Score = {auroc_result:.3f}\")\n",
    "    \n",
    "    RocCurveDisplay.from_predictions(y_test, y_prob_test[:, 1])  # Use probabilities for ROC curve\n",
    "    plt.title(\"AUROC: Decision Tree\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    print(f\"\\n\\nInterpreting the decision tree: if the condition in the box (node) is TRUE, take the LEFT branch. If FALSE, take the RIGHT.\\n\")\n",
    "    plot_tree(pipe[\"clf\"], \n",
    "              feature_names=features[:-1],\n",
    "              class_names=[\"Other\", target_genre],\n",
    "              filled=True,\n",
    "              impurity=False,\n",
    "              rounded=True,\n",
    "              fontsize=14\n",
    "              )\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265ec28-6ad9-4bb4-9b0c-284f494f2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_dt(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test, features):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test = dt_binary(X_train, \n",
    "                                                                     X_test, \n",
    "                                                                     y_train, \n",
    "                                                                     y_test, \n",
    "                                                                     target_genre,\n",
    "                                                                     features)\n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,\"pred_train\"] = y_pred_train\n",
    "    df_new.loc[indices_test,\"pred_test\"] = y_pred_test\n",
    "    df_new.loc[indices_train,\"prob_0_train\"] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,\"prob_0_test\"] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,\"prob_1_train\"] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,\"prob_1_test\"] = y_prob_test[:,1]    \n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96195ad4-befc-4622-a836-131060d8416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_preds_df = genres_binary_dt(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3161c-ff18-4318-a6f2-9a0f5587d86d",
   "metadata": {},
   "source": [
    "## Inspect incorrectly classified texts\n",
    "\n",
    "We can explore which texts were incorrectly classified by the two models. Run the cell below to display dataframes of the misclassified texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644d8eb-0dcb-4d59-81cd-690205cfd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "lr_misclass = lr_preds_df.loc[(lr_preds_df[\"binary_class\"] != lr_preds_df[\"pred_test\"]) & (lr_preds_df[\"pred_test\"] >= 0)]\n",
    "lr_misclass = lr_misclass.filter([\"date\", \n",
    "                                  \"newspaper_id\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"article_id\", \n",
    "                                  \"title\", \n",
    "                                  \"text\", \n",
    "                                  \"clean_text\",\n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"article_web\"], \n",
    "                                  axis=1\n",
    "                                ).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Logistic Regression model (lr)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(lr_misclass)\n",
    "\n",
    "dt_misclass = dt_preds_df.loc[(dt_preds_df[\"binary_class\"] != dt_preds_df[\"pred_test\"]) & (dt_preds_df[\"pred_test\"] >= 0)]\n",
    "dt_misclass = dt_misclass.filter([\"date\", \n",
    "                                  \"newspaper_id\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"article_id\", \n",
    "                                  \"title\", \n",
    "                                  \"text\", \n",
    "                                  \"clean_text\",\n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"article_web\"], \n",
    "                                  axis=1\n",
    "                                ).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Decision Tree model (dt)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(dt_misclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599ce01-c104-494b-8532-75f222572c7a",
   "metadata": {},
   "source": [
    "### Display the full text, the feature values, and the newspaper web address of a selected misclassification by model and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cb5b8-6a6c-4d80-875b-b8482c49d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and index number of the misclassified text\n",
    "\n",
    "# Enter 'lr' or 'dt'\n",
    "model_code = \"lr\"\n",
    "selected_index = 0\n",
    "\n",
    "##################################################################\n",
    "\n",
    "if model_code == \"lr\":\n",
    "    article_title = f\"{lr_misclass['title'].values[selected_index]}\"\n",
    "    article_text = f\"{lr_misclass['text'].values[selected_index]}\"\n",
    "    print(\"\\nTitle:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"{article_title}\")\n",
    "    print(\"\\nText:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_text)\n",
    "    print(\"\\nCleaned text:\")\n",
    "    print(\"--------------\")\n",
    "    print(lr_misclass[\"clean_text\"].values[selected_index])\n",
    "    print(\"\\nView the scanned article on the Papers Past website\")\n",
    "    print(lr_misclass['article_web'].values[selected_index]) \n",
    "\n",
    "elif model_code == \"dt\":\n",
    "    article_title = f\"{dt_misclass['title'].values[selected_index]}\"\n",
    "    article_text = f\"{dt_misclass['text'].values[selected_index]}\"\n",
    "    print(\"\\nTitle:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"{article_title}\")\n",
    "    print(\"\\nOriginal OCR text:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_text)\n",
    "    print(\"\\nCleaned text:\")\n",
    "    print(\"--------------\")\n",
    "    print(dt_misclass[\"clean_text\"].values[selected_index])\n",
    "    print(\"\\nView the scanned article on the Papers Past website\")\n",
    "    print(f\"{dt_misclass['article_web'].values[selected_index]}\\n\") \n",
    "else:\n",
    "    print(\"\\nPlease enter either 'lr' or 'dt' for the model code\")\n",
    "    \n",
    "mask = (df['title'] == article_title) & (df['text'] == article_text)\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-strand",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> Examine some of the misclassified texts. Why do you think they were misclassified? Examine the coefficients of the logistic regression model or the decision tree nodes, and the feature values. You can compare the feature values for the important logistic regression features in selected article to the overall dataset distribution for that feature shown in the summary statistics (see cell below). Discuss with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect summary statistics for the whole dataset to compare to the misclassified text\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
